{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdcbdbf6-5c2d-4083-9ccb-3b181d47915a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T08:16:10.533453Z",
     "iopub.status.busy": "2023-10-13T08:16:10.533453Z",
     "iopub.status.idle": "2023-10-13T08:16:14.060117Z",
     "shell.execute_reply": "2023-10-13T08:16:14.058114Z",
     "shell.execute_reply.started": "2023-10-13T08:16:10.533453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot find .env file\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Provider | Environment variable | Set? | Models |\n",
       "|----------|----------------------|------|--------|\n",
       "| `ai21` | `AI21_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `ai21:j1-large`, `ai21:j1-grande`, `ai21:j1-jumbo`, `ai21:j1-grande-instruct`, `ai21:j2-large`, `ai21:j2-grande`, `ai21:j2-jumbo`, `ai21:j2-grande-instruct`, `ai21:j2-jumbo-instruct` |\n",
       "| `bedrock` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | `bedrock:amazon.titan-tg1-large`, `bedrock:anthropic.claude-v1`, `bedrock:anthropic.claude-instant-v1`, `bedrock:anthropic.claude-v2`, `bedrock:ai21.j2-jumbo-instruct`, `bedrock:ai21.j2-grande-instruct` |\n",
       "| `anthropic` | `ANTHROPIC_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `anthropic:claude-v1`, `anthropic:claude-v1.0`, `anthropic:claude-v1.2`, `anthropic:claude-2`, `anthropic:claude-instant-v1`, `anthropic:claude-instant-v1.0` |\n",
       "| `azure-chat-openai` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | This provider does not define a list of models. |\n",
       "| `cohere` | `COHERE_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `cohere:medium`, `cohere:xlarge` |\n",
       "| `gpt4all` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | `gpt4all:ggml-gpt4all-j-v1.2-jazzy`, `gpt4all:ggml-gpt4all-j-v1.3-groovy`, `gpt4all:ggml-gpt4all-l13b-snoozy` |\n",
       "| `huggingface_hub` | `HUGGINGFACEHUB_API_TOKEN` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | See https://huggingface.co/models for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`. |\n",
       "| `openai` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | `openai:text-davinci-003`, `openai:text-davinci-002`, `openai:text-curie-001`, `openai:text-babbage-001`, `openai:text-ada-001`, `openai:davinci`, `openai:curie`, `openai:babbage`, `openai:ada` |\n",
       "| `openai-chat` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | `openai-chat:gpt-3.5-turbo`, `openai-chat:gpt-3.5-turbo-16k`, `openai-chat:gpt-3.5-turbo-0301`, `openai-chat:gpt-3.5-turbo-0613`, `openai-chat:gpt-3.5-turbo-16k-0613`, `openai-chat:gpt-4`, `openai-chat:gpt-4-0314`, `openai-chat:gpt-4-0613`, `openai-chat:gpt-4-32k`, `openai-chat:gpt-4-32k-0314`, `openai-chat:gpt-4-32k-0613` |\n",
       "| `openai-chat-new` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | `openai-chat-new:gpt-3.5-turbo`, `openai-chat-new:gpt-3.5-turbo-16k`, `openai-chat-new:gpt-3.5-turbo-0301`, `openai-chat-new:gpt-3.5-turbo-0613`, `openai-chat-new:gpt-3.5-turbo-16k-0613`, `openai-chat-new:gpt-4`, `openai-chat-new:gpt-4-0314`, `openai-chat-new:gpt-4-0613`, `openai-chat-new:gpt-4-32k`, `openai-chat-new:gpt-4-32k-0314`, `openai-chat-new:gpt-4-32k-0613` |\n",
       "| `sagemaker-endpoint` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | Specify an endpoint name as the model ID. In addition, you must include the `--region_name`, `--request_schema`, and the `--response_path` arguments. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints). |\n",
       "\n",
       "Aliases and custom commands:\n",
       "\n",
       "| Name | Target |\n",
       "|------|--------|\n",
       "| `gpt2` | `huggingface_hub:gpt2` |\n",
       "| `gpt3` | `openai:text-davinci-003` |\n",
       "| `chatgpt` | `openai-chat:gpt-3.5-turbo` |\n",
       "| `gpt4` | `openai-chat:gpt-4` |\n",
       "| `titan` | `bedrock:amazon.titan-tg1-large` |\n"
      ],
      "text/plain": [
       "ai21\n",
       "Requires environment variable AI21_API_KEY (not set)\n",
       "* ai21:j1-large\n",
       "* ai21:j1-grande\n",
       "* ai21:j1-jumbo\n",
       "* ai21:j1-grande-instruct\n",
       "* ai21:j2-large\n",
       "* ai21:j2-grande\n",
       "* ai21:j2-jumbo\n",
       "* ai21:j2-grande-instruct\n",
       "* ai21:j2-jumbo-instruct\n",
       "\n",
       "bedrock\n",
       "* bedrock:amazon.titan-tg1-large\n",
       "* bedrock:anthropic.claude-v1\n",
       "* bedrock:anthropic.claude-instant-v1\n",
       "* bedrock:anthropic.claude-v2\n",
       "* bedrock:ai21.j2-jumbo-instruct\n",
       "* bedrock:ai21.j2-grande-instruct\n",
       "\n",
       "anthropic\n",
       "Requires environment variable ANTHROPIC_API_KEY (not set)\n",
       "* anthropic:claude-v1\n",
       "* anthropic:claude-v1.0\n",
       "* anthropic:claude-v1.2\n",
       "* anthropic:claude-2\n",
       "* anthropic:claude-instant-v1\n",
       "* anthropic:claude-instant-v1.0\n",
       "\n",
       "azure-chat-openai\n",
       "Requires environment variable OPENAI_API_KEY (set)\n",
       "* This provider does not define a list of models.\n",
       "\n",
       "cohere\n",
       "Requires environment variable COHERE_API_KEY (not set)\n",
       "* cohere:medium\n",
       "* cohere:xlarge\n",
       "\n",
       "gpt4all\n",
       "* gpt4all:ggml-gpt4all-j-v1.2-jazzy\n",
       "* gpt4all:ggml-gpt4all-j-v1.3-groovy\n",
       "* gpt4all:ggml-gpt4all-l13b-snoozy\n",
       "\n",
       "huggingface_hub\n",
       "Requires environment variable HUGGINGFACEHUB_API_TOKEN (not set)\n",
       "* See https://huggingface.co/models for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`.\n",
       "\n",
       "openai\n",
       "Requires environment variable OPENAI_API_KEY (set)\n",
       "* openai:text-davinci-003\n",
       "* openai:text-davinci-002\n",
       "* openai:text-curie-001\n",
       "* openai:text-babbage-001\n",
       "* openai:text-ada-001\n",
       "* openai:davinci\n",
       "* openai:curie\n",
       "* openai:babbage\n",
       "* openai:ada\n",
       "\n",
       "openai-chat\n",
       "Requires environment variable OPENAI_API_KEY (set)\n",
       "* openai-chat:gpt-3.5-turbo\n",
       "* openai-chat:gpt-3.5-turbo-16k\n",
       "* openai-chat:gpt-3.5-turbo-0301\n",
       "* openai-chat:gpt-3.5-turbo-0613\n",
       "* openai-chat:gpt-3.5-turbo-16k-0613\n",
       "* openai-chat:gpt-4\n",
       "* openai-chat:gpt-4-0314\n",
       "* openai-chat:gpt-4-0613\n",
       "* openai-chat:gpt-4-32k\n",
       "* openai-chat:gpt-4-32k-0314\n",
       "* openai-chat:gpt-4-32k-0613\n",
       "\n",
       "openai-chat-new\n",
       "Requires environment variable OPENAI_API_KEY (set)\n",
       "* openai-chat-new:gpt-3.5-turbo\n",
       "* openai-chat-new:gpt-3.5-turbo-16k\n",
       "* openai-chat-new:gpt-3.5-turbo-0301\n",
       "* openai-chat-new:gpt-3.5-turbo-0613\n",
       "* openai-chat-new:gpt-3.5-turbo-16k-0613\n",
       "* openai-chat-new:gpt-4\n",
       "* openai-chat-new:gpt-4-0314\n",
       "* openai-chat-new:gpt-4-0613\n",
       "* openai-chat-new:gpt-4-32k\n",
       "* openai-chat-new:gpt-4-32k-0314\n",
       "* openai-chat-new:gpt-4-32k-0613\n",
       "\n",
       "sagemaker-endpoint\n",
       "* Specify an endpoint name as the model ID. In addition, you must include the `--region_name`, `--request_schema`, and the `--response_path` arguments. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints).\n",
       "\n",
       "\n",
       "Aliases and custom commands:\n",
       "gpt2 - huggingface_hub:gpt2\n",
       "gpt3 - openai:text-davinci-003\n",
       "chatgpt - openai-chat:gpt-3.5-turbo\n",
       "gpt4 - openai-chat:gpt-4\n",
       "titan - bedrock:amazon.titan-tg1-large\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jupyter_ai\n",
    "%reload_ext jupyter_ai_magics\n",
    "%reload_ext dotenv\n",
    "%dotenv /root/key.env\n",
    "%ai list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d2902cc-7e62-489e-bba5-7991dd0ee2bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T08:18:16.146642Z",
     "iopub.status.busy": "2023-10-13T08:18:16.145642Z",
     "iopub.status.idle": "2023-10-13T08:20:06.466399Z",
     "shell.execute_reply": "2023-10-13T08:20:06.464528Z",
     "shell.execute_reply.started": "2023-10-13T08:18:16.146642Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "以下是使用PII数据集训练GCN模型的示例代码：\n",
       "\n",
       "```python\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.optim as optim\n",
       "from torch.utils.data import DataLoader\n",
       "from torch.utils.data.dataset import Dataset\n",
       "import numpy as np\n",
       "from sklearn.model_selection import train_test_split\n",
       "from sklearn.preprocessing import LabelEncoder\n",
       "\n",
       "# 定义GCN模型\n",
       "class GCN(nn.Module):\n",
       "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
       "        super(GCN, self).__init__()\n",
       "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
       "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
       "\n",
       "    def forward(self, x, adj_matrix):\n",
       "        x = torch.relu(self.fc1(torch.matmul(adj_matrix, x)))\n",
       "        x = self.fc2(torch.matmul(adj_matrix, x))\n",
       "        return torch.softmax(x, dim=-1)\n",
       "\n",
       "# 定义PII数据集类\n",
       "class PIIDataset(Dataset):\n",
       "    def __init__(self, data, labels):\n",
       "        self.data = data\n",
       "        self.labels = labels\n",
       "\n",
       "    def __getitem__(self, index):\n",
       "        x = self.data[index]\n",
       "        y = self.labels[index]\n",
       "        return x, y\n",
       "\n",
       "    def __len__(self):\n",
       "        return len(self.data)\n",
       "\n",
       "# 加载PII数据集\n",
       "data = np.load('pii_data.npy')  # PII数据\n",
       "labels = np.load('pii_labels.npy')  # 标签\n",
       "\n",
       "# 标签编码\n",
       "label_encoder = LabelEncoder()\n",
       "labels = label_encoder.fit_transform(labels)\n",
       "\n",
       "# 划分训练集和测试集\n",
       "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
       "\n",
       "# 构建数据加载器\n",
       "train_dataset = PIIDataset(X_train, y_train)\n",
       "test_dataset = PIIDataset(X_test, y_test)\n",
       "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
       "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
       "\n",
       "# 训练函数\n",
       "def train(model, criterion, optimizer, num_epochs):\n",
       "    for epoch in range(num_epochs):\n",
       "        model.train()\n",
       "        running_loss = 0.0\n",
       "        for inputs, labels in train_loader:\n",
       "            inputs = inputs.float()\n",
       "            labels = labels.long()\n",
       "\n",
       "            optimizer.zero_grad()\n",
       "\n",
       "            outputs = model(inputs, adj_matrix)  # 使用GCN进行前向传播\n",
       "\n",
       "            loss = criterion(outputs, labels)\n",
       "            loss.backward()\n",
       "            optimizer.step()\n",
       "            running_loss += loss.item()\n",
       "\n",
       "        print(f\"Epoch: {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
       "\n",
       "# 创建GCN模型\n",
       "input_dim = data.shape[1]  # 输入维度\n",
       "hidden_dim = 64  # 隐藏层维度\n",
       "output_dim = len(label_encoder.classes_)  # 输出维度，即标签类别数\n",
       "\n",
       "model = GCN(input_dim, hidden_dim, output_dim)\n",
       "\n",
       "# 定义损失函数和优化器\n",
       "criterion = nn.CrossEntropyLoss()\n",
       "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
       "\n",
       "# 开始训练\n",
       "num_epochs = 30\n",
       "\n",
       "train(model, criterion, optimizer, num_epochs)\n",
       "```\n",
       "\n",
       "请注意，上述代码中的`adj_matrix`是标准化的邻接矩阵，它应根据具体的图结构进行构建。请根据自己的数据集和图结构相应地修改代码。此外，还需根据实际数据集的特征维度和类别数调整模型的输入和输出维度。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "text/markdown": {
       "jupyter_ai": {
        "model_id": "gpt-3.5-turbo",
        "provider_id": "openai-chat"
       }
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ai openai-chat:gpt-3.5-turbo\n",
    "请帮我写一个使用PII数据集训练GCN模型的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a07659-7e36-49a2-8981-8d48d6704502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
